# clean_diabetic_data.ipynb
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Data Cleaning Pipeline\n",
    "This notebook implements the full data cleaning pipeline for the ITS 2122 group project.\n",
    "It loads the raw diabetic dataset, handles missing values, removes expired patients, merges ID mappings, and saves the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os" 
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Configuration" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "SCRIPT_DIR = os.getcwd()  # Assuming notebook is in project root\n",
    "RAW_DATA_PATH = os.path.join(SCRIPT_DIR, 'data', 'raw', 'diabetic_data.csv')\n",
    "ID_MAP_PATH = os.path.join(SCRIPT_DIR, 'data', 'raw', 'IDs_mapping.csv')\n",
    "OUTPUT_PATH = os.path.join(SCRIPT_DIR, 'data', 'processed', 'diabetic_data_clean.csv')"],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_data(path=RAW_DATA_PATH):\n",
    "    df = pd.read_csv(path, na_values=['?'])\n",
    "    print(f'Loaded data with shape: {df.shape}')\n",
    "    return df\n",
    "\n",
    "def save_clean_data(df, path=OUTPUT_PATH):\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f'Saved cleaned data to {path}')\n",
    "\n",
    "def drop_columns_if_missing(df, threshold=0.9, columns=None):\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    cols_to_drop = [col for col in columns if df[col].isna().mean() > threshold]\n",
    "    if cols_to_drop:\n",
    "        print(f'Dropping columns due to missing values > {threshold*100}%: {cols_to_drop}')\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    return df\n",
    "\n",
    "def remove_expired_patients(df, expired_ids=[11]):\n",
    "    if 'discharge_disposition_id' not in df.columns:\n",
    "        print('Column discharge_disposition_id not found.')\n",
    "        return df\n",
    "    initial_count = len(df)\n",
    "    df = df[~df['discharge_disposition_id'].isin(expired_ids)]\n",
    "    print(f'Removed {initial_count - len(df)} expired patients.')\n",
    "    return df\n",
    "\n",
    "def merge_id_descriptions(df, mapping_df, id_col, new_col_name):\n",
    "    mapping_df = mapping_df[['id', 'description']].drop_duplicates(subset='id')\n",
    "    df = df.merge(mapping_df, how='left', left_on=id_col, right_on='id')\n",
    "    df = df.drop(columns=['id'])\n",
    "    df = df.rename(columns={'description': new_col_name})\n",
    "    print(f'Merged descriptions for {id_col} into {new_col_name}. Row count: {len(df)}')\n",
    "    return df\n",
    "\n",
    "def load_mapping_sections(mapping_csv_path):\n",
    "    sections = {'admission_type_id': [], 'discharge_disposition_id': [], 'admission_source_id': []}\n",
    "    current_section = None\n",
    "    with open(mapping_csv_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith('admission_type_id'):\n",
    "                current_section = 'admission_type_id'\n",
    "                continue\n",
    "            elif line.startswith('discharge_disposition_id'):\n",
    "                current_section = 'discharge_disposition_id'\n",
    "                continue\n",
    "            elif line.startswith('admission_source_id'):\n",
    "                current_section = 'admission_source_id'\n",
    "                continue\n",
    "            if ',' not in line or current_section is None:\n",
    "                continue\n",
    "            id_val, desc = line.split(',', 1)\n",
    "            if id_val.isdigit():\n",
    "                sections[current_section].append({'id': int(id_val), 'description': desc.strip('"')})\n",
    "    admission_type_df = pd.DataFrame(sections['admission_type_id'])\n",
    "    discharge_df = pd.DataFrame(sections['discharge_disposition_id'])\n",
    "    admission_source_df = pd.DataFrame(sections['admission_source_id'])\n",
    "    return admission_type_df, discharge_df, admission_source_df" ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution" ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def clean_diabetic_data():\n",
    "    df = load_data()\n",
    "    print(f'Initial shape: {df.shape}')\n",
    "    df = drop_columns_if_missing(df, threshold=0.9, columns=['weight'])\n",
    "    print(f'After dropping weight: {df.shape}')\n",
    "    df = remove_expired_patients(df, expired_ids=[11,19,20,21])\n",
    "    print(f'After removing expired patients: {df.shape}')\n",
    "    admission_type_map, discharge_map, admission_source_map = load_mapping_sections(ID_MAP_PATH)\n",
    "    df = merge_id_descriptions(df, admission_type_map, 'admission_type_id', 'admission_type_desc')\n",
    "    df = merge_id_descriptions(df, discharge_map, 'discharge_disposition_id', 'discharge_desc')\n",
    "    df = merge_id_descriptions(df, admission_source_map, 'admission_source_id', 'admission_source_desc')\n",
    "    df_before = df.shape[0]\n",
    "    df = df.drop_duplicates()\n",
    "    print(f'Removed {df_before - df.shape[0]} duplicate rows')\n",
    "    save_clean_data(df)\n",
    "    return df\n",
    "\n",
    "# Run pipeline\n",
    "cleaned_df = clean_diabetic_data()\n",
    "cleaned_df.head()" ]
  }
 ],
 "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}},
 "nbformat": 4,
 "nbformat_minor": 5
}
