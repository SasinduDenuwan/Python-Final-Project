{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd13132b",
   "metadata": {},
   "source": [
    "# Diabetic Data Cleaning Notebook\n",
    "This notebook performs Phase 1 of the data cleaning process for the diabetic dataset. It loads the raw data, handles missing values, removes expired patients, merges ID descriptions, and validates the final cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1685d4de",
   "metadata": {},
   "source": [
    "## 1️⃣ Load the raw diabetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths (adjust if needed)\n",
    "RAW_DATA_PATH = '../data/raw/diabetic_data.csv'\n",
    "ID_MAP_PATH = '../data/raw/IDs_mapping.csv'\n",
    "OUTPUT_PATH = '../data/processed/diabetic_data_clean.csv'\n",
    "\n",
    "# Load raw data, converting '?' to NaN\n",
    "df = pd.read_csv(RAW_DATA_PATH, na_values=['?'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0589f42",
   "metadata": {},
   "source": [
    "## 2️⃣ Drop `weight` column due to excessive missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df['weight'].isna().mean() > 0.9:\n",
    "    df = df.drop(columns=['weight'])\n",
    "    print('Dropped weight column due to >90% missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db138ea",
   "metadata": {},
   "source": [
    "## 3️⃣ Remove expired patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expired_ids = [11,19,20,21]  # Expired discharge_disposition_id\n",
    "df = df[~df['discharge_disposition_id'].isin(expired_ids)]\n",
    "print(f'Remaining rows after removing expired patients: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d53c9",
   "metadata": {},
   "source": [
    "## 4️⃣ Load and merge ID mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- 4️⃣ Load and merge ID mappings ---------------- #\n",
    "\n",
    "# Function to parse the IDs_mapping.csv into separate DataFrames\n",
    "def load_mapping_sections(mapping_csv_path):\n",
    "    \"\"\"\n",
    "    Parse IDs_mapping.csv which contains multiple mapping tables\n",
    "    separated by header rows and blank lines.\n",
    "    Returns three DataFrames: admission_type, discharge_disposition, admission_source\n",
    "    \"\"\"\n",
    "    sections = {\n",
    "        \"admission_type_id\": [],\n",
    "        \"discharge_disposition_id\": [],\n",
    "        \"admission_source_id\": []\n",
    "    }\n",
    "\n",
    "    current_section = None\n",
    "\n",
    "    with open(mapping_csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Detect section headers\n",
    "            if line.startswith(\"admission_type_id\"):\n",
    "                current_section = \"admission_type_id\"\n",
    "                continue\n",
    "            elif line.startswith(\"discharge_disposition_id\"):\n",
    "                current_section = \"discharge_disposition_id\"\n",
    "                continue\n",
    "            elif line.startswith(\"admission_source_id\"):\n",
    "                current_section = \"admission_source_id\"\n",
    "                continue\n",
    "\n",
    "            # Skip malformed lines\n",
    "            if \",\" not in line or current_section is None:\n",
    "                continue\n",
    "\n",
    "            id_val, desc = line.split(\",\", 1)\n",
    "            if id_val.isdigit():\n",
    "                sections[current_section].append(\n",
    "                    {\"id\": int(id_val), \"description\": desc.strip('\"')}\n",
    "                )\n",
    "\n",
    "    # Convert each section to DataFrame\n",
    "    admission_type_df = pd.DataFrame(sections[\"admission_type_id\"])\n",
    "    discharge_df = pd.DataFrame(sections[\"discharge_disposition_id\"])\n",
    "    admission_source_df = pd.DataFrame(sections[\"admission_source_id\"])\n",
    "\n",
    "    return admission_type_df, discharge_df, admission_source_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mapping CSV\n",
    "admission_type_map, discharge_map, admission_source_map = load_mapping_sections(ID_MAP_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99e71c",
   "metadata": {},
   "source": [
    "### Note: The mapping CSV contains multiple sections. For simplicity, we'll create separate mappings manually based on the earlier CSV content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fad32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map numeric IDs to descriptions\n",
    "def merge_id_descriptions_safe(df, mapping_df, id_col, new_col_name):\n",
    "    \"\"\"\n",
    "    Maps numeric IDs to descriptions safely without creating extra columns.\n",
    "    \"\"\"\n",
    "    mapping_dict = mapping_df.set_index('id')['description'].to_dict()\n",
    "    df[new_col_name] = df[id_col].map(mapping_dict)\n",
    "    print(f\"Mapped '{id_col}' to '{new_col_name}'\")\n",
    "    return df\n",
    "\n",
    "# Merge all three mappings into the main DataFrame\n",
    "df = merge_id_descriptions_safe(df, admission_type_map, 'admission_type_id', 'admission_type_desc')\n",
    "df = merge_id_descriptions_safe(df, discharge_map, 'discharge_disposition_id', 'discharge_desc')\n",
    "df = merge_id_descriptions_safe(df, admission_source_map, 'admission_source_id', 'admission_source_desc')\n",
    "\n",
    "# Check results\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c9064",
   "metadata": {},
   "source": [
    "## 5️⃣ Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_count = len(df)\n",
    "df = df.drop_duplicates()\n",
    "print(f'Removed {initial_count - len(df)} duplicate rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19077882",
   "metadata": {},
   "source": [
    "## 6️⃣ Validate cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429eba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1131e30b",
   "metadata": {},
   "source": [
    "## 7️⃣ Save cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff064da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f'Saved cleaned dataset to {OUTPUT_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1994bf7c",
   "metadata": {},
   "source": [
    "## Cleaning Decisions Summary:\n",
    "- **Missing values**: Replaced '?' with NaN for proper handling.\n",
    "- **Weight column**: Dropped due to >90% missing values.\n",
    "- **Expired patients**: Removed patients with `discharge_disposition_id` 11.\n",
    "- **ID mappings**: Merged human-readable descriptions for admission type, discharge disposition, and admission source.\n",
    "- **Duplicates**: Removed duplicate rows to ensure clean data.\n",
    "- **Validation**: Dataset info and descriptive stats checked post-cleaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
